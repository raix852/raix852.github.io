---
title: 7/29 痞客邦克漏字比賽紀錄
date: 2016-7-29 23:10:37
tags: [Machine Learning, Deep Learning, NLP]
---

最近這陣子都熱心在學習 Deep learning 的東西，動機來自於和同事組隊參加 `2016 PIXNET HACKATHON Cloze Contest`。這個比賽會給出一句話，裡面會挖一個洞，並給出 4 個選項來選出符合這個洞的答案，也可以說是傳統的選擇題考試。對從小到大都是考試機器的我們來說是最常見不過的問題，因此都練出的一身本領來回答。但是對於機器來說，要選出正確答案還真不是件簡單的事，更可說是巨大的挑戰。

因此為了可以讓機器有辦法回答，需要有一些技巧來使用。以下紀錄目前我和同事做到的進度。正式比賽是在8/13 日，離比賽還有兩星期。

## 爬取文章
在這個比賽裡，訓練資料集放出來是在 7/01 時，而我們準備的時間從 6 月初就開始，因此在沒有資料的情況下，我們就決定要開發爬網程式將痞客邦的文章抓取下來。這部分使用 [Scrapy](http://scrapy.org/) 來幫助我們開發爬網程式。

目前做到的成果為針對每天有被痞客邦視為熱門的文章抓取下來，並且順便將這些文章的作者曾經寫過的文章也一併抓取。跑一次爬網程式需要花費半天的時間，目前累積了將近 2G 的資料。不過現在的機制只是堪用，要做進一步改善還有很大的空間，只是考慮到接下來還有更困難的問題要解決，因此先開發到這邊。

## 斷詞斷句
將文章抓取下來後，接下來使用 [Jieba](https://github.com/fxsjy/jieba) 來幫助我們做斷詞。這裏就直接使用現成的工具，因此不是太困難的工作。當然使用過程中會看到很多奇奇怪怪的字詞出現，這就需要人工介入將一些字詞濾掉。

## Word2vec
將上面斷出來的字詞整理出一個字詞庫出來，接下來要做的動作是把字詞變成向量。根據 Word2vec 演算法，字詞轉成向量後會有一些有趣的相關性，像是 `國王 - 男人 + 女人 = 皇后`。因此要完成這次的競賽，其實用 Word2vec 就可以做到不錯的成果，把選項的向量拿去和題目比對相似度，把最高相似度的選項當作正確答案，就會是一個不錯的解題模型。目前有組員利用這個方法做到 6 成正確率。

## RNN/LSTM
這部分就是真正的挑戰啦，對於我這個初學者而言，要跟很多不熟悉的演算法和公式搏鬥。而且要處理部落格文章並拿去訓練也很不容易。基本上要面對龐大的字詞庫，動輒幾十萬的字詞，要算出每個字的機率出來，光想就覺得困難重重。目前這塊還在努力中，希望可以真的玩出一個模型出來。

## 感想
在這個比賽裡，由於接觸到的東西很多且都頗有挑戰性，所以做起來其實滿快樂的。學到很多東西，感覺很過癮。在目前工作上覺得進展有點停滯的時刻，弄這些東西真的有如及時雨一般滋潤我的內心，邊做邊感到持續有在進步中。希望到時能做出不錯的結果出來！
